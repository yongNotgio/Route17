{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Route25 Dataset - Scrape Full Guide Pages\n",
        "\n",
        "        This notebook enriches the dataset by scraping each `full_guide_url`\n",
        "        found in `output/iloilo_routes_index.json`.\n",
        "\n",
        "        It exports:\n",
        "        - `output/iloilo_full_guides.json`\n",
        "        - `output/iloilo_full_guides_summary.csv`\n"
      ],
      "id": "98e9d147"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "from urllib.parse import parse_qs, urljoin, urlparse\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "OUTPUT_DIR = Path(\"output\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "INDEX_JSON = OUTPUT_DIR / \"iloilo_routes_index.json\"\n",
        "if not INDEX_JSON.exists():\n",
        "    raise FileNotFoundError(\"Run 01_scrape_route_index.ipynb first.\")\n",
        "\n",
        "KML_URL_TEMPLATE = \"https://www.google.com/maps/d/kml?mid={mid}&forcekml=1\"\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
        "    \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
        "}\n"
      ],
      "id": "56af48b0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_text(text: str) -> str:\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    return \" \".join(text.replace(\"\\xa0\", \" \").split())\n",
        "\n",
        "def unique_in_order(items):\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for item in items:\n",
        "        if item and item not in seen:\n",
        "            seen.add(item)\n",
        "            out.append(item)\n",
        "    return out\n",
        "\n",
        "def extract_article_dates(soup: BeautifulSoup):\n",
        "    date_published = None\n",
        "    date_modified = None\n",
        "\n",
        "    for script in soup.find_all(\"script\", attrs={\"type\": \"application/ld+json\"}):\n",
        "        raw = script.string or script.get_text(strip=True)\n",
        "        if not raw:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            payload = json.loads(raw)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        candidates = []\n",
        "        if isinstance(payload, dict):\n",
        "            if isinstance(payload.get(\"@graph\"), list):\n",
        "                candidates.extend(payload[\"@graph\"])\n",
        "            candidates.append(payload)\n",
        "        elif isinstance(payload, list):\n",
        "            candidates.extend(payload)\n",
        "\n",
        "        for candidate in candidates:\n",
        "            if not isinstance(candidate, dict):\n",
        "                continue\n",
        "\n",
        "            candidate_type = candidate.get(\"@type\")\n",
        "            if isinstance(candidate_type, list):\n",
        "                type_match = any(t in {\"Article\", \"BlogPosting\", \"NewsArticle\"} for t in candidate_type)\n",
        "            else:\n",
        "                type_match = candidate_type in {\"Article\", \"BlogPosting\", \"NewsArticle\"}\n",
        "\n",
        "            if type_match:\n",
        "                date_published = date_published or candidate.get(\"datePublished\")\n",
        "                date_modified = date_modified or candidate.get(\"dateModified\")\n",
        "\n",
        "        if date_published and date_modified:\n",
        "            break\n",
        "\n",
        "    return date_published, date_modified\n",
        "\n",
        "def extract_mid_from_url(url: str):\n",
        "    if not url:\n",
        "        return None\n",
        "    parsed = urlparse(url)\n",
        "    query = parse_qs(parsed.query)\n",
        "    mids = query.get(\"mid\")\n",
        "    return mids[0] if mids else None\n",
        "\n",
        "def parse_kml_polylines(kml_text: str):\n",
        "    kml_soup = BeautifulSoup(kml_text, \"xml\")\n",
        "    polylines = []\n",
        "\n",
        "    for idx, placemark in enumerate(kml_soup.find_all(\"Placemark\"), start=1):\n",
        "        line = placemark.find(\"LineString\")\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        coordinates_tag = line.find(\"coordinates\")\n",
        "        if not coordinates_tag:\n",
        "            continue\n",
        "\n",
        "        coordinate_tokens = normalize_text(coordinates_tag.get_text(\" \", strip=True)).split()\n",
        "        coordinates_lng_lat = []\n",
        "        coordinates_lat_lng = []\n",
        "\n",
        "        for token in coordinate_tokens:\n",
        "            parts = token.split(\",\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                lng = float(parts[0])\n",
        "                lat = float(parts[1])\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "            coordinates_lng_lat.append([lng, lat])\n",
        "            coordinates_lat_lng.append([lat, lng])\n",
        "\n",
        "        if len(coordinates_lng_lat) < 2:\n",
        "            continue\n",
        "\n",
        "        name_tag = placemark.find(\"name\")\n",
        "        polyline_name = normalize_text(name_tag.get_text(\" \", strip=True)) if name_tag else f\"segment_{idx}\"\n",
        "\n",
        "        polylines.append(\n",
        "            {\n",
        "                \"name\": polyline_name,\n",
        "                \"point_count\": len(coordinates_lng_lat),\n",
        "                \"coordinates_lng_lat\": coordinates_lng_lat,\n",
        "                \"coordinates_lat_lng\": coordinates_lat_lng,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return polylines\n",
        "\n",
        "def fetch_map_geometry(map_mid: str, session: requests.Session, cache: dict):\n",
        "    if not map_mid:\n",
        "        return {\n",
        "            \"map_mid\": None,\n",
        "            \"map_kml_url\": None,\n",
        "            \"map_polylines\": [],\n",
        "            \"map_polyline_count\": 0,\n",
        "            \"map_point_count\": 0,\n",
        "            \"map_scrape_error\": None,\n",
        "        }\n",
        "\n",
        "    if map_mid in cache:\n",
        "        return cache[map_mid]\n",
        "\n",
        "    kml_url = KML_URL_TEMPLATE.format(mid=map_mid)\n",
        "    try:\n",
        "        response = session.get(kml_url, headers=HEADERS, timeout=45)\n",
        "        response.raise_for_status()\n",
        "        if not response.encoding:\n",
        "            response.encoding = \"utf-8\"\n",
        "\n",
        "        map_polylines = parse_kml_polylines(response.text)\n",
        "        result = {\n",
        "            \"map_mid\": map_mid,\n",
        "            \"map_kml_url\": kml_url,\n",
        "            \"map_polylines\": map_polylines,\n",
        "            \"map_polyline_count\": len(map_polylines),\n",
        "            \"map_point_count\": sum(polyline[\"point_count\"] for polyline in map_polylines),\n",
        "            \"map_scrape_error\": None,\n",
        "        }\n",
        "    except Exception as exc:\n",
        "        result = {\n",
        "            \"map_mid\": map_mid,\n",
        "            \"map_kml_url\": kml_url,\n",
        "            \"map_polylines\": [],\n",
        "            \"map_polyline_count\": 0,\n",
        "            \"map_point_count\": 0,\n",
        "            \"map_scrape_error\": str(exc),\n",
        "        }\n",
        "\n",
        "    cache[map_mid] = result\n",
        "    time.sleep(0.35)\n",
        "    return result\n",
        "\n",
        "def scrape_full_guide(route_row: dict, session: requests.Session, map_cache: dict):\n",
        "    url = route_row.get(\"full_guide_url\")\n",
        "    if not url:\n",
        "        return None\n",
        "\n",
        "    response = session.get(url, headers=HEADERS, timeout=30)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    if not response.encoding or response.encoding.lower() == \"iso-8859-1\":\n",
        "        response.encoding = response.apparent_encoding or \"utf-8\"\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"lxml\")\n",
        "    article = soup.select_one(\"article .entry-content\") or soup.select_one(\".entry-content\") or soup\n",
        "\n",
        "    title_tag = soup.find(\"h1\", class_=re.compile(\"entry-title\")) or soup.find(\"h1\")\n",
        "    article_title = normalize_text(title_tag.get_text(\" \", strip=True)) if title_tag else \"\"\n",
        "\n",
        "    canonical_tag = soup.find(\"link\", rel=\"canonical\")\n",
        "    canonical_url = canonical_tag.get(\"href\") if canonical_tag and canonical_tag.get(\"href\") else url\n",
        "\n",
        "    paragraphs = []\n",
        "    for p in article.select(\"p\"):\n",
        "        text = normalize_text(p.get_text(\" \", strip=True))\n",
        "        if not text:\n",
        "            continue\n",
        "        if text.lower().startswith(\"read also\"):\n",
        "            continue\n",
        "        paragraphs.append(text)\n",
        "\n",
        "    headings = [\n",
        "        normalize_text(h.get_text(\" \", strip=True))\n",
        "        for h in article.select(\"h2, h3, h4\")\n",
        "        if normalize_text(h.get_text(\" \", strip=True))\n",
        "    ]\n",
        "\n",
        "    map_embed_urls = unique_in_order([urljoin(url, iframe.get(\"src\")) for iframe in article.select(\"iframe[src]\")])\n",
        "    map_geometry = []\n",
        "    for embed_url in map_embed_urls:\n",
        "        map_mid = extract_mid_from_url(embed_url)\n",
        "        geometry = fetch_map_geometry(map_mid, session=session, cache=map_cache)\n",
        "        map_geometry.append({\"map_embed_url\": embed_url, **geometry})\n",
        "\n",
        "    date_published, date_modified = extract_article_dates(soup)\n",
        "\n",
        "    return {\n",
        "        \"route_number\": route_row.get(\"route_number\"),\n",
        "        \"route_title\": route_row.get(\"route_title\"),\n",
        "        \"full_guide_url\": url,\n",
        "        \"canonical_url\": canonical_url,\n",
        "        \"article_title\": article_title,\n",
        "        \"date_published\": date_published,\n",
        "        \"date_modified\": date_modified,\n",
        "        \"first_paragraph\": paragraphs[0] if paragraphs else None,\n",
        "        \"paragraphs\": paragraphs,\n",
        "        \"headings\": headings,\n",
        "        \"map_embed_urls\": map_embed_urls,\n",
        "        \"map_geometry\": map_geometry,\n",
        "        \"guide_polyline_count\": sum(item[\"map_polyline_count\"] for item in map_geometry),\n",
        "        \"guide_point_count\": sum(item[\"map_point_count\"] for item in map_geometry),\n",
        "        \"scraped_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
        "    }\n"
      ],
      "id": "cf9bd34c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "index_payload = json.loads(INDEX_JSON.read_text(encoding=\"utf-8\"))\n",
        "routes = index_payload.get(\"routes\", [])\n",
        "\n",
        "full_guide_candidates = [r for r in routes if r.get(\"full_guide_url\")]\n",
        "print(f\"Routes with full guide URL: {len(full_guide_candidates)}\")\n"
      ],
      "id": "306e97f1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "full_guides = []\n",
        "errors = []\n",
        "\n",
        "session = requests.Session()\n",
        "map_cache = {}\n",
        "\n",
        "for route in full_guide_candidates:\n",
        "    try:\n",
        "        record = scrape_full_guide(route, session=session, map_cache=map_cache)\n",
        "        if record:\n",
        "            full_guides.append(record)\n",
        "        time.sleep(0.6)\n",
        "    except Exception as exc:\n",
        "        errors.append(\n",
        "            {\n",
        "                \"route_number\": route.get(\"route_number\"),\n",
        "                \"route_title\": route.get(\"route_title\"),\n",
        "                \"full_guide_url\": route.get(\"full_guide_url\"),\n",
        "                \"error\": str(exc),\n",
        "            }\n",
        "        )\n",
        "\n",
        "session.close()\n",
        "\n",
        "print(f\"Guides scraped: {len(full_guides)}\")\n",
        "print(f\"Errors: {len(errors)}\")\n",
        "print(f\"Guides with geometry: {sum(1 for g in full_guides if g['guide_polyline_count'] > 0)}\")\n"
      ],
      "id": "16057363"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "full_guides_path = OUTPUT_DIR / \"iloilo_full_guides.json\"\n",
        "payload = {\n",
        "    \"source\": \"shemaegomez full guide pages\",\n",
        "    \"scraped_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
        "    \"guide_count\": len(full_guides),\n",
        "    \"guides_with_geometry\": sum(1 for guide in full_guides if guide.get(\"guide_polyline_count\", 0) > 0),\n",
        "    \"total_polyline_segments\": sum(guide.get(\"guide_polyline_count\", 0) for guide in full_guides),\n",
        "    \"total_polyline_points\": sum(guide.get(\"guide_point_count\", 0) for guide in full_guides),\n",
        "    \"error_count\": len(errors),\n",
        "    \"guides\": full_guides,\n",
        "    \"errors\": errors,\n",
        "}\n",
        "\n",
        "full_guides_path.write_text(json.dumps(payload, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "\n",
        "summary_rows = []\n",
        "for guide in full_guides:\n",
        "    summary_rows.append(\n",
        "        {\n",
        "            \"route_number\": guide.get(\"route_number\"),\n",
        "            \"route_title\": guide.get(\"route_title\"),\n",
        "            \"full_guide_url\": guide.get(\"full_guide_url\"),\n",
        "            \"article_title\": guide.get(\"article_title\"),\n",
        "            \"date_published\": guide.get(\"date_published\"),\n",
        "            \"date_modified\": guide.get(\"date_modified\"),\n",
        "            \"paragraph_count\": len(guide.get(\"paragraphs\") or []),\n",
        "            \"heading_count\": len(guide.get(\"headings\") or []),\n",
        "            \"map_embed_count\": len(guide.get(\"map_embed_urls\") or []),\n",
        "            \"guide_polyline_count\": guide.get(\"guide_polyline_count\"),\n",
        "            \"guide_point_count\": guide.get(\"guide_point_count\"),\n",
        "        }\n",
        "    )\n",
        "\n",
        "pd.DataFrame(summary_rows).to_csv(\n",
        "    OUTPUT_DIR / \"iloilo_full_guides_summary.csv\",\n",
        "    index=False,\n",
        "    encoding=\"utf-8\",\n",
        ")\n",
        "\n",
        "features = []\n",
        "for guide in full_guides:\n",
        "    for map_item in guide.get(\"map_geometry\", []):\n",
        "        for segment_index, polyline in enumerate(map_item.get(\"map_polylines\", []), start=1):\n",
        "            if len(polyline.get(\"coordinates_lng_lat\", [])) < 2:\n",
        "                continue\n",
        "            features.append(\n",
        "                {\n",
        "                    \"type\": \"Feature\",\n",
        "                    \"properties\": {\n",
        "                        \"route_number\": guide.get(\"route_number\"),\n",
        "                        \"route_title\": guide.get(\"route_title\"),\n",
        "                        \"full_guide_url\": guide.get(\"full_guide_url\"),\n",
        "                        \"map_mid\": map_item.get(\"map_mid\"),\n",
        "                        \"segment_index\": segment_index,\n",
        "                        \"segment_name\": polyline.get(\"name\"),\n",
        "                        \"point_count\": polyline.get(\"point_count\"),\n",
        "                    },\n",
        "                    \"geometry\": {\n",
        "                        \"type\": \"LineString\",\n",
        "                        \"coordinates\": polyline.get(\"coordinates_lng_lat\", []),\n",
        "                    },\n",
        "                }\n",
        "            )\n",
        "\n",
        "geojson_path = OUTPUT_DIR / \"iloilo_full_guides_polylines.geojson\"\n",
        "geojson_payload = {\"type\": \"FeatureCollection\", \"features\": features}\n",
        "geojson_path.write_text(json.dumps(geojson_payload, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Saved: {full_guides_path}\")\n",
        "print(\"Saved: output/iloilo_full_guides_summary.csv\")\n",
        "print(f\"Saved: {geojson_path}\")\n"
      ],
      "id": "73210ee7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.DataFrame(\n",
        "    [\n",
        "        {\n",
        "            \"route_number\": g[\"route_number\"],\n",
        "            \"article_title\": g[\"article_title\"],\n",
        "            \"paragraph_count\": len(g[\"paragraphs\"]),\n",
        "            \"map_embed_count\": len(g[\"map_embed_urls\"]),\n",
        "            \"guide_polyline_count\": g[\"guide_polyline_count\"],\n",
        "            \"guide_point_count\": g[\"guide_point_count\"],\n",
        "        }\n",
        "        for g in full_guides\n",
        "    ]\n",
        ").sort_values(\"route_number\").head(15)\n"
      ],
      "id": "932349c8"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}